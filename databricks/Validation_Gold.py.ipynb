{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99aad8d0-9a9a-4772-9930-14781384ebf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Libs\n",
    "from pyspark.sql.functions import col, count, when, isnan, lit, current_date\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76ee871-dd33-4ddc-a0a0-a1cefdeb1208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Datalake connection\n",
    "blobAccessKey = dbutils.secrets.get(scope = \"myscope\", key = \"accesskey\")\n",
    "spark.conf.set(\"fs.azure.account.key.datalakeetlproject.dfs.core.windows.net\", \n",
    "               blobAccessKey) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e7fc9b-ad1f-4bfd-9a2a-c6b898400698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths da camada Gold\n",
    "path_gold_company = \"abfss://gold@datalakeetlproject.dfs.core.windows.net/dim_company\"\n",
    "path_gold_date = \"abfss://gold@datalakeetlproject.dfs.core.windows.net/dim_date\"\n",
    "path_gold_fact = \"abfss://gold@datalakeetlproject.dfs.core.windows.net/fact_quote\"\n",
    "\n",
    "# Read data\n",
    "df_company = spark.read.format(\"delta\").load(path_gold_company)\n",
    "df_date = spark.read.format(\"delta\").load(path_gold_date)\n",
    "df_fact = spark.read.format(\"delta\").load(path_gold_fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c649179c-8b90-4dab-8009-12833e53d93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_nulls(df):\n",
    "    null_counts = df.select([\n",
    "        count(\n",
    "            when(\n",
    "                col(c).isNull() |\n",
    "                ((col(c) == \"\") & (df.schema[c].dataType.simpleString() == \"string\")) |\n",
    "                (isnan(col(c)) if df.schema[c].dataType.simpleString() in [\"double\", \"float\"] else False),\n",
    "                c\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "    return null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a3b5b3-6105-47e5-9750-60950c227fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_dataset(df, dataset_name, critical_columns=[]):\n",
    "    print(f\"Starting data quality validation for: {dataset_name}\")\n",
    "    print(f\"Total records: {df.count()}\")\n",
    "\n",
    "    # Null counts\n",
    "    null_counts_df = count_nulls(df)\n",
    "    print(\"Null counts per column:\")\n",
    "    null_counts_df.show(truncate=False)\n",
    "\n",
    "    # Save data quality report\n",
    "    quality_report = (\n",
    "        null_counts_df\n",
    "        .withColumn(\"total_records\", lit(df.count()))\n",
    "        .withColumn(\"dataset_name\", lit(dataset_name))\n",
    "        .withColumn(\"validation_date\", lit(current_date()))\n",
    "    )\n",
    "\n",
    "    # Raise an exception if critical columns contain nulls\n",
    "    row = null_counts_df.collect()[0]\n",
    "    for col_name in critical_columns:\n",
    "        if row[col_name] > 0:\n",
    "            raise Exception(f\"Validation failed: column '{col_name}' in dataset '{dataset_name}' contains {row[col_name]} null values!\")\n",
    "\n",
    "    print(f\"Validation for {dataset_name} completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fc87d3-6b93-464d-b886-1d3acc2bedf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validate_dataset(df_company, \"dim_company\", critical_columns=[\"ticker\", \"company_name\"])\n",
    "validate_dataset(df_date, \"dim_date\", critical_columns=[\"date\"])\n",
    "validate_dataset(df_fact, \"fact_quote\", critical_columns=[\"ticker\", \"date\", \"open\", \"close\", \"high\", \"low\", \"volume\"])\n",
    "\n",
    "print(\"All Gold layer datasets validated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Validation_Gold.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
